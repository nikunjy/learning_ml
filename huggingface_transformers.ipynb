{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HO5X9gl_PWz"
      },
      "source": [
        "# Transformers, what can they do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN8DmQy6_PW0"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l39xDaYw_PW0"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Just using Pipelines"
      ],
      "metadata": {
        "id": "dpc4Z-JN_8pl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKCujEpS_PW0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6ynwGya_PW0",
        "outputId": "9f806d22-8a8b-41fa-a422-23a99fcd5e94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier(\n",
        "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NcGQ81W_PW0",
        "outputId": "77bdcc8c-72ee-43ec-e95a-b3cf1a15d2e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sequence': 'This is a course about the Transformers library',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"This is a course about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jws7ImBo_PW1",
        "outputId": "f3c9a076-7a26-4f21-efeb-ac1902f6e883"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to understand and use '\n",
              "                    'data flow and data interchange when handling user data. We '\n",
              "                    'will be working with one or more of the most commonly used '\n",
              "                    'data flows â€” data flows of various types, as seen by the '\n",
              "                    'HTTP'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD3ckGUB_PW1",
        "outputId": "058b3ccf-6dfe-4df1-add1-1d1c8df94631"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to manipulate the world and '\n",
              "                    'move your mental and physical capabilities to your advantage.'},\n",
              " {'generated_text': 'In this course, we will teach you how to become an expert and '\n",
              "                    'practice realtime, and with a hands on experience on both real '\n",
              "                    'time and real'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3bYwf1T_PW1",
        "outputId": "079778b2-674b-4228-d687-79a684784d9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'sequence': 'This course will teach you all about mathematical models.',\n",
              "  'score': 0.19619831442832947,\n",
              "  'token': 30412,\n",
              "  'token_str': ' mathematical'},\n",
              " {'sequence': 'This course will teach you all about computational models.',\n",
              "  'score': 0.04052725434303284,\n",
              "  'token': 38163,\n",
              "  'token_str': ' computational'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPWOKnR6_PW1",
        "outputId": "8c27d3ec-641b-47ab-d46f-ac375ab4dc7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, \n",
              " {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, \n",
              " {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n",
              "]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUJc8_KP_PW1",
        "outputId": "0d3ec4c1-b58d-4d13-a9d3-579e90e2dda3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2_yyjCO_PW1",
        "outputId": "d8418af0-987e-4084-cc61-4484d7138ff5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'summary_text': ' America has changed dramatically during recent years . The '\n",
              "                  'number of engineering graduates in the U.S. has declined in '\n",
              "                  'traditional engineering disciplines such as mechanical, civil '\n",
              "                  ', electrical, chemical, and aeronautical engineering . Rapidly '\n",
              "                  'developing economies such as China and India, as well as other '\n",
              "                  'industrial countries in Europe and Asia, continue to encourage '\n",
              "                  'and advance engineering .'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkCB5XCc_PW1",
        "outputId": "bb0a45e8-12cf-4918-fc1b-a875bee44905"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'This course is produced by Hugging Face.'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoTokenizer and Automodel"
      ],
      "metadata": {
        "id": "n87Z3-yaADcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device : \", device)\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "inputs = tokenizer(\"I've been waiting for a HuggingFace course my whole life.\", padding=True, truncation=True, return_tensors=\"pt\")\n",
        "inputs.to(device)\n",
        "print(\"Input shape is 1 X number of tokens : \", inputs[\"input_ids\"].shape)\n",
        "#print(inputs[\"input_ids\"])\n",
        "\n",
        "# AutoModel is a general model that can pull any model from huggingface hub\n",
        "# In this case we are pulling a bert which will output embeddings for each token\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "model.to(device)\n",
        "outputs = model(**inputs)\n",
        "print(\"Output last hidden state is one vector of embodding per token :\", outputs.last_hidden_state.shape)\n",
        "\n",
        "# Let's pull a model for sequence classification. So it will basically add\n",
        "# a head with two outputs one for positive and one for negative. This does\n",
        "# not have a hidden state anymore\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "model.to(device)\n",
        "outputs = model(**inputs)\n",
        "print(\"Outputs : \", outputs.logits.shape)\n",
        "\n",
        "# last output is logits which is basically not actual values but log of actual\n",
        "# values so we can softmax to convert to probabilites. Let's leave that"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z3pEabmAHCe",
        "outputId": "06bfe377-b1b1-48f8-b58f-4ed2d3139f9e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device :  cuda\n",
            "Input shape is 1 X number of tokens :  torch.Size([1, 16])\n",
            "Output last hidden state is one vector of embodding per token : torch.Size([1, 16, 768])\n",
            "Outputs :  torch.Size([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizers"
      ],
      "metadata": {
        "id": "tOfZ35C9DmKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Small lesson on tokenizers\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"yo hahah\",\n",
        "    \"a red fox jumps over the wall and eats your apple\"\n",
        "]\n",
        "\n",
        "batch = []\n",
        "for sentence in sentences:\n",
        "  # Instead of using tokenizer() directly let's break it down to see what all goes\n",
        "  # behind it\n",
        "  inputs = tokenizer.tokenize(sentence)\n",
        "  print(inputs)\n",
        "\n",
        "  # convert to numbers\n",
        "  input_ids  = tokenizer.convert_tokens_to_ids(inputs)\n",
        "  print(input_ids)\n",
        "  batch.append(input_ids)\n",
        "\n",
        "  # this is what goes in the model\n",
        "  # notice how we convert it into a batch because that is what a model takes\n",
        "  actual_input = torch.tensor([input_ids])\n",
        "  print(\"Input IDs:\", actual_input)\n",
        "\n",
        "  # this will work on the batch of tensors\n",
        "  output = model(actual_input)\n",
        "  print(output.logits)\n",
        "  print(\"=============\")\n",
        "\n",
        "# Batching, truncation and attention mask\n",
        "# batch[0] is smaller than batch[1] make it equal length\n",
        "original_length = len(batch[0])\n",
        "batch[0] = batch[0] + [tokenizer.pad_token_id] * (len(batch[1]) - len(batch[0]))\n",
        "print(batch)\n",
        "batched = torch.tensor(batch)\n",
        "print(model(batched).logits)\n",
        "\n",
        "# the output for the first sentence in the batch is different this is because of\n",
        "# attention mask\n",
        "attention_mask = torch.ones(batched.shape[0], batched.shape[1])\n",
        "attention_mask[0, original_length:] = 0\n",
        "print(\"\\nAfter attention mask\")\n",
        "print(model(batched, attention_mask=attention_mask).logits)\n",
        "\n",
        "\n",
        "# When we pass the input to tokenizer directly it can do all the things\n",
        "# like padding and truncate and attention mask\n",
        "print(\"\\nUsing tokenizer directly\")\n",
        "input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(input['input_ids'])\n",
        "print(input['attention_mask'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbVSDcocDnng",
        "outputId": "0b85d140-3b40-4622-ca64-c16598cb2a20"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yo', 'ha', '##ha', '##h']\n",
            "[10930, 5292, 3270, 2232]\n",
            "Input IDs: tensor([[10930,  5292,  3270,  2232]])\n",
            "tensor([[ 1.8348, -1.5315]], grad_fn=<AddmmBackward0>)\n",
            "=============\n",
            "['a', 'red', 'fox', 'jumps', 'over', 'the', 'wall', 'and', 'eats', 'your', 'apple']\n",
            "[1037, 2417, 4419, 14523, 2058, 1996, 2813, 1998, 20323, 2115, 6207]\n",
            "Input IDs: tensor([[ 1037,  2417,  4419, 14523,  2058,  1996,  2813,  1998, 20323,  2115,\n",
            "          6207]])\n",
            "tensor([[-1.6322,  1.7704]], grad_fn=<AddmmBackward0>)\n",
            "=============\n",
            "[[10930, 5292, 3270, 2232, 0, 0, 0, 0, 0, 0, 0], [1037, 2417, 4419, 14523, 2058, 1996, 2813, 1998, 20323, 2115, 6207]]\n",
            "tensor([[ 0.6134, -0.6137],\n",
            "        [-1.6322,  1.7704]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "After attention mask\n",
            "tensor([[ 1.8348, -1.5315],\n",
            "        [-1.6322,  1.7704]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Using tokenizer directly\n",
            "tensor([[  101, 10930,  5292,  3270,  2232,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0],\n",
            "        [  101,  1037,  2417,  4419, 14523,  2058,  1996,  2813,  1998, 20323,\n",
            "          2115,  6207,   102]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Transformers, what can they do?",
      "provenance": [],
      "gpuType": "V100"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}